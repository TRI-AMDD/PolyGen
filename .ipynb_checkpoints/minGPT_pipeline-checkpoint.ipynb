{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5593892c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/Users/weikeye/miniconda3/envs/polygen/lib/python3.8/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mrdkit\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chem\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeepchem\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeat\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msmiles_tokenizer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SmilesTokenizer\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpolygen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpolygen\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/polygen/lib/python3.8/site-packages/deepchem/feat/smiles_tokenizer.py:10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpkg_resources\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m List, Optional\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BertTokenizer\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m getLogger\n\u001b[1;32m     13\u001b[0m logger \u001b[38;5;241m=\u001b[39m getLogger(\u001b[38;5;18m__name__\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from rdkit import Chem\n",
    "import sys\n",
    "from deepchem.feat.smiles_tokenizer import SmilesTokenizer\n",
    "from polygen.metrics import *\n",
    "from polygen.dataset import *\n",
    "from polygen.pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c824f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_col: mol_smiles\n",
      "length: 5\n",
      "block_size: 64\n",
      "train_test_split: (0.8, 0.2)\n",
      "task: conditional\n",
      "file_path: ./htp_md.csv\n",
      "\n",
      "number of parameters: 0.12M\n",
      "auto\n",
      "ckpts_path: None\n",
      "num_samples: 100\n",
      "temperature: 1.0\n",
      "task: conditional\n",
      "ckpt_path: ./ckpts/10000.pt\n",
      "\n",
      "(0.9299999999999999, 0.54, 0.9, 0.9090909090909091, 0.2610139264039455, 0.7002973161769398)\n"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT\n",
    "\n",
    "# Data preprocessing\n",
    "pipeline = minGPT()\n",
    "data_config = pipeline.get_default_data_config()\n",
    "data_config.file_path = \"./htp_md.csv\"\n",
    "data_config.block_size = 64\n",
    "\n",
    "print(data_config)\n",
    "train_dataset, test_dataset = pipeline.data_preprocessing(data_config)\n",
    "\n",
    "\n",
    "# Model\n",
    "model_config = pipeline.get_default_model_config()\n",
    "model_config.model_type = 'gpt-nano'\n",
    "model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "model_config.block_size = train_dataset.get_block_size()\n",
    "pipeline.load_model(model_config)\n",
    "\n",
    "# Train\n",
    "train_config = pipeline.get_default_train_config()\n",
    "\n",
    "print(train_config.device)\n",
    "train_config.max_iters = 10000\n",
    "train_config.ckpt_path = \"./ckpts/\"\n",
    "# train_config.pretrain = \"./ckpts/10000.pt\"\n",
    "## Define call back function\n",
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}, val loss {trainer.loss_val.item():.5f}\")\n",
    "\n",
    "train_config.call_back = batch_end_callback\n",
    "# loss = pipeline.train(train_config)\n",
    "\n",
    "# Generate\n",
    "generate_config = pipeline.get_default_generate_config()\n",
    "generate_config.ckpt_path = \"./ckpts/10000.pt\"\n",
    "assert generate_config.task == data_config.task\n",
    "print(generate_config)\n",
    "\n",
    "results = pipeline.generate(generate_config)\n",
    "\n",
    "# Evaluate\n",
    "print(pipeline.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf23e5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polygen",
   "language": "python",
   "name": "polygen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
