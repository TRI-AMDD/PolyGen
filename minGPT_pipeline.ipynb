{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5593892c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/weikeye/miniconda3/envs/polygen/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Skipped loading some Tensorflow models, missing a dependency. No module named 'tensorflow'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. No module named 'torch_geometric'\n",
      "Skipped loading modules with pytorch-geometric dependency, missing a dependency. cannot import name 'DMPNN' from 'deepchem.models.torch_models' (/Users/weikeye/miniconda3/envs/polygen/lib/python3.8/site-packages/deepchem/models/torch_models/__init__.py)\n",
      "Skipped loading modules with pytorch-lightning dependency, missing a dependency. No module named 'pytorch_lightning'\n",
      "Skipped loading some Jax models, missing a dependency. No module named 'jax'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset\n",
    "from rdkit import Chem\n",
    "import sys\n",
    "from deepchem.feat.smiles_tokenizer import SmilesTokenizer\n",
    "from polygen.metrics import *\n",
    "from polygen.dataset import *\n",
    "from polygen.pipeline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c824f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_col: mol_smiles\n",
      "length: 5\n",
      "block_size: 64\n",
      "train_test_split: (0.8, 0.2)\n",
      "task: conditional\n",
      "file_path: ./htp_md.csv\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Can't find a vocabulary file at path 'vocab.txt'. To load the vocabulary from a Google pretrained model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 10\u001B[0m\n\u001B[1;32m      7\u001B[0m data_config\u001B[38;5;241m.\u001B[39mblock_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m64\u001B[39m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(data_config)\n\u001B[0;32m---> 10\u001B[0m train_dataset, test_dataset \u001B[38;5;241m=\u001B[39m \u001B[43mpipeline\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata_preprocessing\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata_config\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;66;03m# Model\u001B[39;00m\n\u001B[1;32m     14\u001B[0m model_config \u001B[38;5;241m=\u001B[39m pipeline\u001B[38;5;241m.\u001B[39mget_default_model_config()\n",
      "File \u001B[0;32m~/repos/PolyGen/polygen/pipeline.py:95\u001B[0m, in \u001B[0;36mminGPT.data_preprocessing\u001B[0;34m(self, data_config)\u001B[0m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdata_preprocessing\u001B[39m(\u001B[38;5;28mself\u001B[39m, data_config):\n\u001B[0;32m---> 95\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtokenizer \u001B[38;5;241m=\u001B[39m \u001B[43mSmilesTokenizer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvocab_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mvocab.txt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(data_config\u001B[38;5;241m.\u001B[39mfile_path, sep\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \n\u001B[1;32m     97\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata_num \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf)\n",
      "File \u001B[0;32m~/miniconda3/envs/polygen/lib/python3.8/site-packages/deepchem/feat/smiles_tokenizer.py:86\u001B[0m, in \u001B[0;36mSmilesTokenizer.__init__\u001B[0;34m(self, vocab_file, **kwargs)\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m     69\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     70\u001B[0m     vocab_file: \u001B[38;5;28mstr\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     75\u001B[0m     \u001B[38;5;66;03m# mask_token=\"[MASK]\",\u001B[39;00m\n\u001B[1;32m     76\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m     77\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Constructs a SmilesTokenizer.\u001B[39;00m\n\u001B[1;32m     78\u001B[0m \n\u001B[1;32m     79\u001B[0m \u001B[38;5;124;03m  Parameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     83\u001B[0m \u001B[38;5;124;03m    Default vocab file is found in deepchem/feat/tests/data/vocab.txt\u001B[39;00m\n\u001B[1;32m     84\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m---> 86\u001B[0m   \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mvocab_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     88\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(vocab_file):\n\u001B[1;32m     89\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m     90\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find a vocab file at path \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(vocab_file))\n",
      "File \u001B[0;32m~/miniconda3/envs/polygen/lib/python3.8/site-packages/transformers/models/bert/tokenization_bert.py:200\u001B[0m, in \u001B[0;36mBertTokenizer.__init__\u001B[0;34m(self, vocab_file, do_lower_case, do_basic_tokenize, never_split, unk_token, sep_token, pad_token, cls_token, mask_token, tokenize_chinese_chars, strip_accents, **kwargs)\u001B[0m\n\u001B[1;32m    184\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m    185\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    186\u001B[0m     vocab_file,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    197\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    198\u001B[0m ):\n\u001B[1;32m    199\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39misfile(vocab_file):\n\u001B[0;32m--> 200\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    201\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCan\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mt find a vocabulary file at path \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mvocab_file\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m. To load the vocabulary from a Google pretrained\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    202\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    203\u001B[0m         )\n\u001B[1;32m    204\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab \u001B[38;5;241m=\u001B[39m load_vocab(vocab_file)\n\u001B[1;32m    205\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mids_to_tokens \u001B[38;5;241m=\u001B[39m collections\u001B[38;5;241m.\u001B[39mOrderedDict([(ids, tok) \u001B[38;5;28;01mfor\u001B[39;00m tok, ids \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab\u001B[38;5;241m.\u001B[39mitems()])\n",
      "\u001B[0;31mValueError\u001B[0m: Can't find a vocabulary file at path 'vocab.txt'. To load the vocabulary from a Google pretrained model use `tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)`"
     ]
    }
   ],
   "source": [
    "from mingpt.model import GPT\n",
    "\n",
    "# Data preprocessing\n",
    "pipeline = minGPT()\n",
    "data_config = pipeline.get_default_data_config()\n",
    "data_config.file_path = \"polygen/htp_md.csv\"\n",
    "data_config.block_size = 64\n",
    "\n",
    "print(data_config)\n",
    "train_dataset, test_dataset = pipeline.data_preprocessing(data_config)\n",
    "\n",
    "\n",
    "# Model\n",
    "model_config = pipeline.get_default_model_config()\n",
    "model_config.model_type = 'gpt-nano'\n",
    "model_config.vocab_size = train_dataset.get_vocab_size()\n",
    "model_config.block_size = train_dataset.get_block_size()\n",
    "pipeline.load_model(model_config)\n",
    "\n",
    "# Train\n",
    "train_config = pipeline.get_default_train_config()\n",
    "\n",
    "print(train_config.device)\n",
    "train_config.max_iters = 10000\n",
    "train_config.ckpt_path = \"./ckpts/\"\n",
    "# train_config.pretrain = \"./ckpts/10000.pt\"\n",
    "## Define call back function\n",
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}, val loss {trainer.loss_val.item():.5f}\")\n",
    "\n",
    "train_config.call_back = batch_end_callback\n",
    "# loss = pipeline.train(train_config)\n",
    "\n",
    "# Generate\n",
    "generate_config = pipeline.get_default_generate_config()\n",
    "generate_config.ckpt_path = \"./ckpts/10000.pt\"\n",
    "assert generate_config.task == data_config.task\n",
    "print(generate_config)\n",
    "\n",
    "results = pipeline.generate(generate_config)\n",
    "\n",
    "# Evaluate\n",
    "print(pipeline.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf23e5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polygen",
   "language": "python",
   "name": "polygen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}